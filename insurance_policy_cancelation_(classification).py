# -*- coding: utf-8 -*-
"""Insurance Policy Cancelation (Classification).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1khYgMjbUKf41azrwoFOnRZsgVbo2eqPB

# Project: Insurance Policy Cancellation (CC)
---------------------------------------------

*This dataset comes from the 2022 NESS Statathon (Theme 1) from 2022.*

**Challenge:** Using historical policy data, create a multiclass predictive model to predict the policies that are most likely to be canceled and those most likely to be renewed, as well as understand what variables are most influential in causing a policy cancellation.

For this theme, there are true answers, and a team should focus on proposing the best predictive model. The performance of a team will be mainly based on the predictive performance of the propose method measured by accuracy and the quality of the code. You can use Python's weighted F1 score to calculate the performance of for your model on the test partition.

# Dataset Description
The Kangaroo data set is based on 4 years of property insurance policies from 2013 to 2017. There are roughly 1 million policies in the training data and each policy only has one observation. There were almost 230,000 policies canceled during the effective term. Your work is to build a model on the training data and use your best model to predict the cancelation indicator for each policy in test data.

Variable descriptions
* id - Policy id (cannot be used in model)
* tenure - Number of years with Kangaroo
* claim.ind - Occurrence of claim (0=no, 1=yes)
* n.adults - Number of adults in the property
* n.children - Number of children in the property
* ni.gender - Gender of policyholder
* ni.marital.status - Marital status of policyholder (0=no, 1=yes)
* premium - Price of the policy
* sales.channel - Medium through which policy was purchased
* coverage.type - Type of coverage
* dwelling.type - Type of dwelling
* len.at.res - Length at residence (how long policyholder lived at property)
* credit - Financial credit level of policyholder
* house.color - Color of house
* ni.age - Age of policholder
* year - Year of the policy
* zip.code - Zip code of the property
* cancel - cancelation indicator (0=not cancel, 1=may cancel but can be convinced, 2=cancel). **This is the response variable.** Dave is deleting the rows where `cancel==-1`!
"""

!pip install pyspark

from pyspark.sql import SparkSession
spark = SparkSession.builder\
        .appName("Project_3")\
        .getOrCreate()

# our usual pyspark friends
from pyspark.sql.utils import AnalysisException
import pyspark.sql.functions as F
import pyspark.sql.types as T
from functools import reduce

# pandas
import pandas as pd

!pip install --upgrade gdown

#train_df : https://drive.google.com/file/d/1ALi1coNsEFepkvB_19YGNMjd89OdtqwA/view?usp=share_link
#test_df : https://drive.google.com/file/d/1AISuYnBX7X-joDfO_Byi_MXrRl4tg2FJ/view?usp=share_link

!gdown 1ALi1coNsEFepkvB_19YGNMjd89OdtqwA
!gdown 1AISuYnBX7X-joDfO_Byi_MXrRl4tg2FJ

train_df = spark.read.option("inferSchema", "true").csv("./train_df_simple (1).csv", multiLine=True, header=True)
test_df  = spark.read.option("inferSchema", "true").csv("./test_df (1).csv", multiLine=True, header=True)

"""## Train"""

train_df.show()

"""## Test"""

test_df.show()

"""#Cleaning

## Cleaning column names
"""

# Knowing the train dataframe
train_pd = pd.read_csv("./train_df_simple (1).csv")
print(train_pd.shape)
train_pd.describe()

"""Here, we clean the column names of both the training and test data set where we replace the '.' in the column names with '_'.

## Train
"""

# Cleaning the column names in training dataset
for col in train_df.columns:
    new_col = col.replace('-', '_').replace('.', '_')
    train_df = train_df.withColumnRenamed(col, new_col)

train_df.show(20, False)

"""## Test"""

# Cleaning the column names in training dataset
for col in test_df.columns:
    new_col = col.replace('-', '_').replace('.', '_')
    test_df = test_df.withColumnRenamed(col, new_col)

test_df.show(20, False)

"""## Cleaning Data

The code written below is to identify and verify the binary columns.
"""

# Getting the binary and verifying them
is_binary = train_df.agg(*[(F.size(F.collect_set(x)) == 2).alias(x) for x in train_df.columns]).toPandas()
is_binary

train_df.printSchema()

"""The below code divides it into binary, nomial and continous."""

# dividing the columns into binary, nomial, and continuous variables
column_details = train_df.agg(*[(F.size(F.collect_set(x))).alias(x) for x in train_df.columns]).toPandas()
column_details

"""I have defined the Identifiers, binary columns, nominal columns, continuous variable and target variable."""

IDENTIFIERS = ['id']
BINARY_COlUMNS = ["ni_gender", "claim_ind", "ni_marital_status"]
NOMINAL_COLUMNS = ["year", "zip_code", "house_color", "credit", "coverage_type",  "dwelling_type", "sales_channel" ]
CONTINUOUS_VARIABLE = ["premium", "ni_age", "len_at_res", "n_adults",	"n_children", "tenure"	 ]
TARGET_VARIABLE = ['cancel']

"""### Deleting all the rows where all the cells are null

Here, I am dropping all the rows in the train and test data set where all cells in the rows are null.

## Train
"""

# deleting all the rows where all the cells are null
train_df = train_df.dropna(
    how="all",
    subset=[x for x in train_df.columns if x not in IDENTIFIERS], # this is just a list of all columns except identifiers
)

print(train_df.count(), len(train_df.columns))

"""From above, I can clearly see that nothing changed, in the number of rows and columns.

## Test
"""

# deleting all the rows where all the cells are null
test_df = test_df.dropna(
    how="all",
    subset=[x for x in test_df.columns if x not in IDENTIFIERS], # this is just a list of all columns except identifiers
)

print(test_df.count(), len(test_df.columns))

"""So, now I know that I cannot drop any row as all of them have some information. Hence, I will go forward with imputation.

The below code is to calculate the number of missing values in each column.
"""

# Calculate the number of missing value in each column
row_missing_values = train_df.select(*[
    (F.sum(F.col(c).isNull().cast('int'))).alias(c)
    for c in train_df.columns
])

row_missing_values.show()

"""# Some Graphs to see the trend between different columns.

### Distribution based on credit and age

This code is to add the dataframe to the table to run sql queries.
"""

query1 = train_df.select('cancel', 'credit','ni_age')
query1.createOrReplaceTempView("data")

"""Adding relavant data into data frame."""

df_high = spark.sql('SELECT ni_age FROM data WHERE credit == "high"').toPandas()
df_med = spark.sql('SELECT ni_age FROM data WHERE credit == "medium"').toPandas()
df_low = spark.sql('SELECT ni_age FROM data WHERE credit == "low"').toPandas()

"""Visualize the histplot."""

import seaborn as sns
import matplotlib.pyplot as plt
sns.histplot(data=df_high, x="ni_age", color="green", label="High Credit", kde=True, alpha = 0.6, )
sns.histplot(data=df_med, x="ni_age", color="yellow", label="Medium Credit", kde=True, alpha = 0.4, )
sns.histplot(data=df_low, x="ni_age", color="red", label="Low Credit", kde=True, alpha = 0.2, )

plt.title("Distribution of Credit Scores by Age")
plt.legend()
plt.show()

"""From the above graph, I can observe that alot of mid aged people have high credit scores. It is also seen that a large chunk of the data set has people with high credit. I also observe that the youngest population has a does not have a high credit.

### Distribution of Age based on Sales Channel

This code is to add the dataframe to the table to run sql queries.
"""

query2 = train_df.select('cancel', 'sales_channel','ni_age')
query2.createOrReplaceTempView("data")

"""Adding relavant data into data frame.

"""

df_broker = spark.sql('SELECT ni_age FROM data WHERE sales_channel == "Broker"').toPandas()
df_phone = spark.sql('SELECT ni_age FROM data WHERE sales_channel == "Phone"').toPandas()
df_online = spark.sql('SELECT ni_age FROM data WHERE sales_channel == "Online"').toPandas()

"""Visualize the histplot."""

import seaborn as sns
import matplotlib.pyplot as plt

fig, ax = plt.subplots()
sns.histplot(data=df_broker, x="ni_age", color="orange", label="Broker", kde=True, alpha=0.8, ax=ax)
sns.histplot(data=df_phone, x="ni_age", color="yellow", label="Phone", kde=True, alpha=0.6, ax=ax)
sns.histplot(data=df_online, x="ni_age", color="green", label="Online", kde=True, alpha=0.4, ax=ax)
ax.set(title="Distribution of Age based on Sales Channel")
ax.legend()
plt.show()

"""Here, I can observe that most of the population prefers "Broker" as the sales channel, followed by "Phone" and then "Online mode". It is also observed that most of the mid aged population prfers their sales channel to be a broker. I see that there is a huge drop between broker and phone as compared to between broker phone and online.

### Cancellation based on claim

This code is to add the dataframe to the table to run sql queries.
"""

query3= train_df.select('claim_ind', 'cancel')
query3.createOrReplaceTempView("data")

"""Adding relavant data into data frame.

"""

df3 = spark.sql('SELECT cancel, SUM(CASE WHEN claim_ind==0 THEN 1 END) AS No, SUM(CASE WHEN claim_ind==1 THEN 1 END) AS Yes FROM data GROUP BY cancel').toPandas()
df3

"""Visualize the bargraph."""

# Calculate percentages
df3['Total'] = df3['No'] + df3['Yes']
df3['No_Percentage'] = 100 * df3['No'] / df3['Total']
df3['Yes_Percentage'] = 100 * df3['Yes'] / df3['Total']

# Set up plot
barWidth = 0.75
names = ('No Cancel', 'Maybe Cancel', 'Cancel')

# Plot bars
plt.bar(df3['cancel'], df3['Yes_Percentage'], color='red', edgecolor='white', width=barWidth, label='Yes')
plt.bar(df3['cancel'], df3['No_Percentage'], bottom=df3['Yes_Percentage'], color='blue', edgecolor='white', width=barWidth, label='No')

# Set axis labels and legend
plt.xticks(df3['cancel'], names)
plt.xlabel("Cancellation Status")
plt.legend(loc='upper right', bbox_to_anchor=(1,1), ncol=1)

# Set title and display plot
plt.title("Portion of Claim by Cancellation Status")
plt.show()

"""From the above graph I can observe that a high proportion of population have their claim status as No for all types of cancellations. The proportion of Yes is low for people who cancel, while it increases and is pretty much same for people who do not cancel and who maybe cancel it.

### Cancellation based on Coverage type

This code is to add the dataframe to the table to run sql queries.
"""

query4= train_df.select('coverage_type', 'cancel')
query4.createOrReplaceTempView("data")

"""Adding relavant data into data frame.

"""

df4 = spark.sql('SELECT cancel, \
                SUM(CASE WHEN coverage_type=="A" THEN 1 END) AS A, \
                SUM(CASE WHEN coverage_type=="B" THEN 1 END) AS B, \
                SUM(CASE WHEN coverage_type=="C" THEN 1 END) AS C \
                FROM data \
                GROUP BY cancel').toPandas()
df4

"""Visualize the bar graph."""

# Calculate percentages
df4['Total'] = df4['A'] + df4['B']+ df4['C']
df4['A_Percentage'] = 100 * df4['A'] / df4['Total']
df4['B_Percentage'] = 100 * df4['B'] / df4['Total']
df4['C_Percentage'] = 100 * df4['C'] / df4['Total']

# Set up plot
barWidth = 0.75
names = ('No Cancel', 'Maybe Cancel', 'Cancel')

# Plot bars
plt.bar(df4['cancel'], df4['A_Percentage'], color='red', edgecolor='white', width=barWidth, label='A')
plt.bar(df4['cancel'], df4['B_Percentage'], color='green', edgecolor='white', width=barWidth, label='B')
plt.bar(df4['cancel'], df4['C_Percentage'], bottom=df4['A_Percentage'], color='yellow', edgecolor='white', width=barWidth, label='C')

# Set axis labels and legend
plt.xticks(df4['cancel'], names)
plt.xlabel("Cancellation Status")
plt.legend(loc='upper right', bbox_to_anchor=(1,1), ncol=1)

# Set title and display plot
plt.title("% of Portion of Coverage type by Cancellation Status")
plt.show()

"""From the above plot I can observe that most of the population has coverage type as C for all options of cancellations. The next common coverage type is "A" but I see a dip in it while going from cancel to no cancel and maybe cancel. The least used coverage type is B which looks distributed much same for all the cancel types.

### Distribution of Age based on Claim

This code is to add the dataframe to the table to run sql queries.
"""

query5 = train_df.select('cancel', 'claim_ind','ni_age')
query5.createOrReplaceTempView("data")

"""Adding relavant data into data frame.

"""

df_no = spark.sql('SELECT ni_age FROM data WHERE claim_ind == 0.0').toPandas()
df_yes = spark.sql('SELECT ni_age FROM data WHERE claim_ind == 1.0').toPandas()

"""Visualize the histplot."""

import seaborn as sns
import matplotlib.pyplot as plt
sns.histplot(data=df_no, x="ni_age", color="green", label="Claim : No", kde=True, alpha = 0.6, )
sns.histplot(data=df_yes, x="ni_age", color="yellow", label="Claim : Yes", kde=True, alpha = 0.4, )

plt.title("Distribution of Age based on Claim")
plt.legend()
plt.show()

"""I can see from the above graph that alot of the middle aged population has the claim as no while comparatively there are a very few yes. It is also observed that there is a large portion of the population have their claim as No.

# Columns

### zip_code

We think that zip codes don't really matter with policy cancelation and it cannot be imputed as there is no logic to it.

#### Train
"""

train_df = train_df.drop("zip_code")

"""#### Test"""

test_df = test_df.drop("zip_code")

"""### house_color

It is not adding any significance to the prediction, hence we decide to drop it.

#### Train
"""

train_df = train_df.drop("house_color")

"""#### Test"""

test_df = test_df.drop("house_color")

"""### ni_age

#### Train

There are policyholders with age>100, which has no logic so I am going to drop those values.
"""

train_df.where(F.col('ni_age')>100).select(F.col('ni_age')).show()

train_df = train_df.filter(F.col('ni_age')<=100)

"""Here, I are going to calculate the average age present in the data set and then replace the null values with this average."""

sum = train_df.select(F.sum(F.col('ni_age')).alias('sum')).show()
count = train_df.select(F.count(F.col('ni_age')).alias('count')).show()

# Calculating the average
avg = 5203357.678989081/119770
print(avg)

#fill null values of age columns
train_df = train_df.fillna(43.44, subset='ni_age')

"""#### Test"""

test_df.where(F.col('ni_age')>100).select(F.col('ni_age')).show()
test_df = test_df.filter(F.col('ni_age')<=100)

sum = test_df.select(F.sum(F.col('ni_age')).alias('sum')).show()
count = test_df.select(F.count(F.col('ni_age')).alias('count')).show()

# Calculating the average
avg = 1.496662345751464E7/344315
print(avg)

#fill null values of age columns
test_df = test_df.fillna(43.46, subset='ni_age')

"""### len_at_res

#### Train

First, I am going to check the minimum and maximum values of len_at_res present in the data set.
"""

train_df.select(F.max(F.col('len_at_res'))).show()
train_df.select(F.min(F.col('len_at_res'))).show()

"""Since there were no out of range values, I will calculate the average value of the len_at_res and then repolace the null values with this average."""

sum_len = train_df.select(F.sum(F.col('len_at_res')).alias('sum_len')).show()
count_len = train_df.select(F.count(F.col('len_at_res')).alias('count_len')).show()

# Calculating the average
avg = 1818620.3752815635/ 119658
print(avg)

#fill null values of age columns
train_df = train_df.fillna(15.19, subset='len_at_res')

"""#### Test"""

test_df.select(F.max(F.col('len_at_res'))).show()
test_df.select(F.min(F.col('len_at_res'))).show()

sum_len = test_df.select(F.sum(F.col('len_at_res')).alias('sum_len')).show()
count_len = test_df.select(F.count(F.col('len_at_res')).alias('count_len')).show()

# Calculating the average
avg = 5234211.782432109/344020
print(avg)

#fill null values of age columns
test_df = test_df.fillna(15.21, subset='len_at_res')

"""### credit

#### Train

I calculate the total count of values present in credit.
"""

train_df.select(F.col('credit')).groupBy('credit').count().orderBy('count', ascending=False).show()

"""The common credit is high, so I am going to impute the missing values with high.

"""

#fill null values of binary columns
train_df = train_df.fillna('high', subset='credit')

"""I have first used the String Indexer before transforming it by OneHotEncoder as this column is of type String.

"""

from pyspark.ml.feature import PolynomialExpansion
from pyspark.mllib.linalg import Vectors
from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import OneHotEncoder, StringIndexer
from pyspark.ml.feature import VectorIndexer
from pyspark.mllib.util import MLUtils
from pyspark.mllib.util import MLUtils
from pyspark.ml.feature import StandardScaler
from pyspark.mllib.linalg import Vectors
from pyspark.ml.feature import VectorAssembler

indexer_credit = StringIndexer(inputCol="credit", outputCol="credit_StInx")
train_df = indexer_credit.fit(train_df).transform(train_df)
encoder_credit = OneHotEncoder(inputCol="credit_StInx", outputCol="credit_OHE")
train_df = encoder_credit.fit(train_df).transform(train_df)

"""#### Test"""

test_df.select(F.col('credit')).groupBy('credit').count().orderBy('count', ascending=False).show()

#fill null values of binary columns
test_df = test_df.fillna('high', subset='credit')

indexer_credit = StringIndexer(inputCol="credit", outputCol="credit_StInx")
test_df = indexer_credit.fit(test_df).transform(test_df)
encoder_credit = OneHotEncoder(inputCol="credit_StInx", outputCol="credit_OHE")
test_df = encoder_credit.fit(test_df).transform(test_df)

"""### coverage_type

#### Train

first, I will calculate the count of values present in coverage_type.
"""

train_df.select(F.col('coverage_type')).groupBy('coverage_type').count().orderBy('count', ascending=False).show()

"""I can see that the most common coverage type is 'C', so I will replace all the null values with "C"."""

train_df = train_df.fillna('C', subset='coverage_type')

"""I have first used the String Indexer before transforming it by OneHotEncoder as this column is of type String.

"""

indexer_coverage_type= StringIndexer(inputCol="coverage_type", outputCol="coverage_type_StInx")
train_df = indexer_coverage_type.fit(train_df).transform(train_df)
encoder_coverage_type = OneHotEncoder(inputCol="coverage_type_StInx", outputCol="coverage_type_OHE")
train_df = encoder_coverage_type.fit(train_df).transform(train_df)

"""#### Test"""

test_df.select(F.col('coverage_type')).groupBy('coverage_type').count().orderBy('count', ascending=False).show()

#fill null values of binary columns
test_df = test_df.fillna('C', subset='coverage_type')

indexer_coverage_type= StringIndexer(inputCol="coverage_type", outputCol="coverage_type_StInx")
test_df = indexer_coverage_type.fit(test_df).transform(test_df)
encoder_coverage_type = OneHotEncoder(inputCol="coverage_type_StInx", outputCol="coverage_type_OHE")
test_df = encoder_coverage_type.fit(test_df).transform(test_df)

"""### dwelling_type

It is not adding any significance to the prediction, hence I decide to drop it.

#### Train
"""

train_df.groupby("dwelling_type").pivot("cancel").count().show()

train_df = train_df.drop("dwelling_type")

"""#### Test"""

test_df = test_df.drop("dwelling_type")

"""### premium

It is not adding any significance to the prediction, hence I decide to drop it.

#### Train
"""

train_df = train_df.drop("premium")

"""#### Test"""

test_df = test_df.drop("premium")

"""### ni_gender

It is not adding any significance to the prediction, hence I decide to drop it.

#### Train
"""

train_df = train_df.drop("ni_gender")

"""#### Test"""

test_df = test_df.drop("ni_gender")

"""### sales_channel

#### Train

First, I will calculate the count of value spresent in sales channel.
"""

train_df.select(F.col('sales_channel')).groupBy('sales_channel').count().orderBy('count', ascending=False).show()

"""I observe that the most common sales channel is Broker, so I will replace the null value with "Broker"."""

#fill null values
train_df = train_df.fillna('Broker', subset='sales_channel')

"""I have first used the String Indexer before transforming it by OneHotEncoder as this column is of type String.

"""

indexer_salesChanel = StringIndexer(inputCol="sales_channel", outputCol="sales_channel_StInx")
train_df = indexer_salesChanel.fit(train_df).transform(train_df)
encoder_salesChanel = OneHotEncoder(inputCol="sales_channel_StInx", outputCol="sales_channel_OHE")
train_df = encoder_salesChanel.fit(train_df).transform(train_df)

"""#### Test"""

test_df.select(F.col('sales_channel')).groupBy('sales_channel').count().orderBy('count', ascending=False).show()

#fill null values
test_df = test_df.fillna('Broker', subset='sales_channel')

indexer_salesChanel = StringIndexer(inputCol="sales_channel", outputCol="sales_channel_StInx")
test_df = indexer_salesChanel.fit(test_df).transform(test_df)
encoder_salesChanel = OneHotEncoder(inputCol="sales_channel_StInx", outputCol="sales_channel_OHE")
test_df = encoder_salesChanel.fit(test_df).transform(test_df)

"""### ni_marital_status

It is not adding any significance to the prediction, hence I decide to drop it.

#### Train
"""

train_df = train_df.drop("ni_marital_status")

"""#### Test"""

test_df = test_df.drop("ni_marital_status")

"""### n_adults

#### Train
"""

train_df.select(F.max(F.col('n_adults'))).show()

"""Here, I will calculate the average number of adults present in the dataset and then replace the null values with this average value."""

sum_adults = train_df.select(F.sum(F.col('n_adults')).alias('sum_adults')).show()
count_adults = train_df.select(F.count(F.col('n_adults')).alias('count_adults')).show()

# Calculating the average
avg = 255464.0/119908
print(avg)

"""Rounding it to 2"""

train_df = train_df.fillna(2.0, subset='n_adults')

"""#### Test"""

test_df.select(F.max(F.col('n_adults'))).show()

sum_adults = test_df.select(F.sum(F.col('n_adults')).alias('sum_adults')).show()
count_adults = test_df.select(F.count(F.col('n_adults')).alias('count_adults')).show()

# Calculating the average
avg = 734146.0/343993
print(avg)

test_df = test_df.fillna(2.0, subset='n_adults')

"""### n_children

#### Train

Here, I will calculate the average number of children present in the dataset and then replace the null values with this average value.
"""

sum_children = train_df.select(F.sum(F.col('n_children')).alias('sum_children')).show()
count_children = train_df.select(F.count(F.col('n_children')).alias('count_children')).show()

# Calculating the average
avg =207593.0 / 119675
print(avg)

"""Rounding 1.7 to 2"""

train_df = train_df.fillna(2.0, subset='n_children')

"""#### Test"""

sum_children = test_df.select(F.sum(F.col('n_children')).alias('sum_children')).show()
count_children = test_df.select(F.count(F.col('n_children')).alias('count_children')).show()

# Calculating the average
avg =596047.0 /343996
print(avg)

test_df = test_df.fillna(2.0, subset='n_children')

"""### tenure

#### Train

Here, I will calculate the average tenure present in the dataset and then replace the null values with this average value.
"""

sum_tenure = train_df.select(F.sum(F.col('tenure')).alias('sum_tenure')).show()
count_tenure = train_df.select(F.count(F.col('tenure')).alias('count_tenure')).show()

# Calculating the average
avg =1394639.0/119646
print(avg)

train_df = train_df.fillna(11.65, subset='tenure')

"""#### Test"""

sum_tenure = test_df.select(F.sum(F.col('tenure')).alias('sum_tenure')).show()
count_tenure = test_df.select(F.count(F.col('tenure')).alias('count_tenure')).show()

# Calculating the average
avg =4013202.0/343992
print(avg)

test_df = test_df.fillna(11.65, subset='tenure')

"""### claim_ind

#### Train

Here, I will first count the values of claim_ind present in the dataset.
"""

train_df.select(F.col('claim_ind')).groupBy('claim_ind').count().orderBy('count', ascending=False).show()

"""I can observe that 0.0 is the most common claim_ind so I will replace the null values with 0.0"""

train_df = train_df.fillna(0.0, subset='claim_ind')

encoder_claim = OneHotEncoder(inputCol="claim_ind", outputCol="claim_ind_OHE")
train_df = encoder_claim.fit(train_df).transform(train_df)

"""#### Test"""

test_df.select(F.col('claim_ind')).groupBy('claim_ind').count().orderBy('count', ascending=False).show()

#fill null values
test_df = test_df.fillna(0.0, subset='claim_ind')

encoder_claim = OneHotEncoder(inputCol="claim_ind", outputCol="claim_ind_OHE")
test_df = encoder_claim.fit(test_df).transform(test_df)

"""# Transformation

Here, I have defind a new train dataset along with continous columns and continous features to perfom the transformation.
I have also used Vector assembler.

## Train
"""

new_train_df = train_df.select('year', "ni_age", "len_at_res", "credit_OHE", "coverage_type_OHE", "sales_channel_OHE", "n_adults", "n_children", "tenure", "claim_ind",'cancel')

CONTINUOUS_COLUMNS = ['year', "ni_age", "len_at_res", "credit_OHE", "coverage_type_OHE", "sales_channel_OHE", "n_adults", "n_children", "tenure", "claim_ind" ]

continuous_features = VectorAssembler(inputCols=CONTINUOUS_COLUMNS, outputCol="continuous_features")

new_train_df = continuous_features.transform(new_train_df)

"""## Test"""

test_df = continuous_features.transform(test_df)

"""# Modeling

I are using the Random Forest Classifier model on the training and testing dataset.
"""

# Import RandomForestClassifier
from pyspark.ml.classification import RandomForestClassifier

# Create model
rfc = RandomForestClassifier(featuresCol='continuous_features', labelCol='cancel', predictionCol='cancelprediction',
                            maxMemoryInMB = 3000, maxDepth = 10, maxBins = 96,
                            bootstrap = True, numTrees = 50)
rfc.setThresholds([3, 1, 2])

rfcmod = rfc.fit(new_train_df)

"""Performing predictions on both the training and testing data set."""

# predictions
train_results = rfcmod.transform(new_train_df)
test_results = rfcmod.transform(test_df)

train_results = train_results.toPandas()

test_results = test_results.toPandas()

"""## Test Results

Below is the code to get test results for testing data set.
"""

from sklearn.metrics import classification_report
print(classification_report(test_results['cancel'], test_results['cancelprediction'], digits=3))

"""## Train Results

Below is the code to get test results for training data set.
"""

from sklearn.metrics import classification_report
print(classification_report(train_results['cancel'], train_results['cancelprediction'], digits=3))

"""# Printing the Tree

Below code helps us to print the tree.
"""

# assuming you have trained a RandomForestClassificationModel named 'rfcmodel'

# access the individual decision trees in the ensemble
trees = rfcmod.trees

# iterate over the trees in the ensemble and print the nodes of each tree
for i, tree in enumerate(trees):
    print(f"Decision Tree #{i + 1}")
    print(tree.toDebugString)

"""# Interpretability and Discussion

On closely calculating the cancellations based on coverage type I found that  coverage A had the highest cancellation percentage at 72% and C had the lowest at 69% , however C saw the most cancellations at 35237 cases and also a may cancel at 11473. This indicates a high dissatisfaction with the coverage type C. Measures must be taken by the company to improve the coverage type C and further introduce marketing strategies like incentivising policy renewals in alignment with the rate of cancellations in all three categories. Furthermore brokers can make sure to put in extra efforts of maintaining an healthy relationship with policy holders of coverage type C . To do an root cause analysis , factors like claims settlement of all the coverage type can be analysed to understand customer behaviour

The insurance company should make a concerted effort to communicate regularly via the brokers with policyholders, providing them with relevant information about their policies, and addressing any concerns they may have.
"""